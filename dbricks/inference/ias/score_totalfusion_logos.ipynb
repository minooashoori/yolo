{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install humanize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"/Workspace/Users/pdacosta@integralads.com/.ide/ctx-inference_stack/src\")\n",
    "sys.path.append(\"/Workspace/Users/pdacosta@integralads.com/.ide/ctx-logoface-detector\")\n",
    "os.environ[\"AWS_PROFILE\"]=\"saml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compute_engine.units.generator import UnitGenerator, UnitDeclatation\n",
    "from compute_engine.structures.entities import RunInfo\n",
    "from compute_engine.structures.messages import MessageHeader, RunInfoMessage, EndOfComputeMessage\n",
    "from compute_engine.structures.entities import FrameDetection, FrameDetections\n",
    "from compute_engine.units.handler import Handler, RunVariable\n",
    "from compute_engine.utils.message_capture import MessageCapture\n",
    "\n",
    "from concurrent.futures import Future\n",
    "import pyarrow.parquet as pq\n",
    "import asyncio\n",
    "import pyarrow as pa\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import concat, lit, col\n",
    "from dbricks.dbutils.boxes import transf_any_box\n",
    "from dbricks.dbutils.paths import innovation_path\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers = 2\n",
    "model_batch_size = 16\n",
    "detection_min_size_percentage = 0.01\n",
    "confidence_threshold = 0.65 # high to be more restrictive\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "generate_uris = True\n",
    "ignore_progress = True\n",
    "\n",
    "\n",
    "TOTAL_FUSION_02_PATH = os.path.join(innovation_path(\"mnt\"), \"pdacosta\", \"data\", \"total_fusion_02\")\n",
    "INPUT_PATH_DATANET = os.path.join(TOTAL_FUSION_02_PATH, \"merged\", \"train_dataset\", \"complete\")\n",
    "\n",
    "\n",
    "input_data_path = INPUT_PATH_DATANET\n",
    "\n",
    "dbfs_mnt_path_output = '/dbfs/mnt/innovation/pdacosta/data/total_fusion_02/merged/train_dataset/logo_predictions'\n",
    "dbfs_mnt_path_uris = '/dbfs/mnt/innovation/pdacosta/data/total_fusion_02/merged/train_dataset/logo_predictions_uris'\n",
    "\n",
    "pyspark_mnt_path_output = dbfs_mnt_path_output.replace('/dbfs', '')\n",
    "pyspark_mnt_path_uris = dbfs_mnt_path_uris.replace('/dbfs', '')\n",
    "\n",
    "s3_prefix = \"s3://\"\n",
    "\n",
    "progress_file = os.path.join(dbfs_mnt_path_uris, 'progress.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "def gen_message(url):\n",
    "    global i\n",
    "    run_info = {}\n",
    "    run_info[\"export\"] = 'json'\n",
    "    run_info[\"pipeline\"] = {\"mode\": \"semi_auto\"}\n",
    "    run_info[\"source\"] = {\n",
    "        \"kind\": \"image\",\n",
    "        \"url\": url,\n",
    "        \"uuid\": \"0\"\n",
    "    }\n",
    "    run_info[\"run\"] = {\n",
    "        \"id\": i\n",
    "    }\n",
    "    i += 1\n",
    "    run_info[\"company\"] = {\"id\": 0}\n",
    "    run_info = RunInfo(**run_info, atomic=None)\n",
    "    return run_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_generator = UnitGenerator(model_engine=\"tf\")\n",
    "unit_generator.set_config(\"database_client\", None)\n",
    "unit_generator.set_config(\"bucket_name\", \"reminiz.production\")\n",
    "unit_generator.set_config(\"cloud_provider\", \"aws\")\n",
    "unit_generator.set_config(\"models_local_path\", \"./\")\n",
    "unit_generator.set_config(\"s3_bucket_url\", \"\")\n",
    "\n",
    "\n",
    "units = []\n",
    "with unit_generator:\n",
    "    input_unit = unit_generator.units.downloader()\n",
    "    x = input_unit\n",
    "    units.append(x)\n",
    "    x @= unit_generator.units.run_frame_extractor()\n",
    "    units.append(x)\n",
    "    x @= unit_generator.units.frame_resizer(target_size=416)\n",
    "    units.append(x)\n",
    "    x @= unit_generator.units.detector(\n",
    "        max_workers= n_workers,\n",
    "        model_path= \"models/detectors/logos/V3-20230116-tf/\",\n",
    "        batchsize= model_batch_size,\n",
    "        detection_min_size_percentage= detection_min_size_percentage,\n",
    "        confidence_threshold=confidence_threshold\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if generate_uris:\n",
    "\n",
    "    df = spark.read.parquet(INPUT_PATH_DATANET).select(\"asset_id\", \"uri\", \"width\", \"height\", \"boxes\").distinct()\n",
    "    df = df.withColumn(\"has_logo\", F.when(F.size(F.col(\"boxes\")) > 0, 1).otherwise(0))\n",
    "    # we only want to images that have logos to replace them\n",
    "    df = df.filter(col(\"has_logo\") == 1)\n",
    "    df = df.drop(\"has_logo\", \"boxes\")\n",
    "    \n",
    "\n",
    "    # create the correct uri for the images, we need to add the s3 prefix to the paths\n",
    "    df = df.withColumn(\"uri\", concat(lit(s3_prefix), col(\"uri\")))\n",
    "    \n",
    "    # filter out the images that are too small, cannot have a dimension smaller than 96 px\n",
    "    df = df.filter((col(\"width\") >= 96) & (col(\"height\") >= 96))\n",
    "    \n",
    "    #keep only the columns we need\n",
    "    df = df.select(\"asset_id\", \"uri\")\n",
    "    \n",
    "    # rename asset_id to asset\n",
    "    df = df.withColumnRenamed(\"asset_id\", \"asset\")\n",
    "    \n",
    "    # read the assets already saved\n",
    "    # if the file does not exist, avoid the error\n",
    "    try:\n",
    "        assets_saved = spark.read.parquet(pyspark_mnt_path_output)\n",
    "        assets_saved = assets_saved.select(\"asset_id\").distinct()\n",
    "        # filter out the assets already saved\n",
    "        df = df.join(assets_saved, on=\"asset\", how=\"left_anti\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # make sure we have no duplicates\n",
    "    df = df.dropDuplicates([\"asset\"])\n",
    "    \n",
    "    #save df as a csv file \n",
    "    df.write.mode(\"overwrite\").csv(os.path.join(pyspark_mnt_path_uris, \"uris.csv\"), header=True)\n",
    "        \n",
    "else:\n",
    "    \n",
    "    df = spark.read.csv(os.path.join(pyspark_mnt_path_uris, \"uris.csv\"), header=True)\n",
    "    \n",
    "\n",
    "# collect the uris and assets into a python lists\n",
    "rows = df.collect()\n",
    "assets, uris = [row.asset for row in rows], [row.uri for row in rows]\n",
    "df.unpersist()\n",
    "\n",
    "print(f\"Number of images to process: {len(assets)}\")\n",
    "print(f\"Assets: {assets[:2]}\")\n",
    "print(f\"Uris: {uris[:2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_batch(assets, uris, boxes, idx):\n",
    "    \n",
    "\n",
    "    data = [{\"asset\":asset, \"uri\": uri, \"boxes\": boxes} for asset, uri, boxes in zip(assets, uris, boxes)]\n",
    "    \n",
    "    df_ = pd.DataFrame(data)\n",
    "    table = pa.Table.from_pandas(df_)\n",
    "    parquet_filename = os.path.join(dbfs_mnt_path_output, f\"logo_boxes_{str(idx).zfill(6)}.parquet\")\n",
    "    try:\n",
    "        pq.write_table(table, parquet_filename)\n",
    "        with open(progress_file, 'w') as f:\n",
    "            f.write(str(idx))\n",
    "    except:\n",
    "        raise Exception('Could not save the parquet file')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ignore_progress:\n",
    "    # read the progress index from a file\n",
    "    try:\n",
    "        with open(progress_file, 'r') as f:\n",
    "            progress = int(f.read())\n",
    "    except:\n",
    "        progress = 0\n",
    "else:\n",
    "    progress = 0\n",
    "\n",
    "n_uris = len(uris)\n",
    "print(f'Progress: {progress} out of {n_uris} images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = input_unit(\n",
    "    run_info=gen_message(\n",
    "        url='s3://mls.us-east-1.innovation/pdacosta/data/logodet_3k/yolo_dataset/images/train/logodet3k_000003.jpg'\n",
    "    )\n",
    ")\n",
    "xyxy_box = f.result()[0][\"frame_detections\"].detections[0].box\n",
    "print(\"xyxy_box\", xyxy_box)\n",
    "xywh_box = transf_any_box(xyxy_box, input_type=\"xyxy\", output_type=\"xywh\")\n",
    "print(\"xywh_box\", xywh_box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.result()\n",
    "# checked that the output is indeed the original image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "\n",
    "# Define a function to plot bounding boxes on an image\n",
    "def plot_bounding_boxes(image, bounding_boxes, scores, threshold=0.5):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image)\n",
    "    ax = plt.gca()\n",
    "\n",
    "    for bbox, score in zip(bounding_boxes, scores):\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        x1 *= image.shape[1]\n",
    "        y1 *= image.shape[0]\n",
    "        x2 *= image.shape[1]\n",
    "        y2 *= image.shape[0]\n",
    "        if score < threshold:\n",
    "            continue\n",
    "        # Create a rectangle patch\n",
    "        rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor='r', facecolor='none')\n",
    "\n",
    "        # Add the rectangle to the current axis\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Example usage with the provided input data\n",
    "input_data = f.result()\n",
    "image = input_data[0]['frame']\n",
    "frame_detections = input_data[0][\"frame_detections\"]\n",
    "detections = frame_detections.detections\n",
    "bounding_boxes = []\n",
    "scores = []\n",
    "for frame_detection in detections:\n",
    "    score = frame_detection.classification[0][-1]\n",
    "    box = frame_detection.box\n",
    "    bounding_boxes.append(box)\n",
    "    scores.append(score)\n",
    "\n",
    "print(list(zip(scores, bounding_boxes)))\n",
    "\n",
    "plot_bounding_boxes(image, bounding_boxes, scores, 0.27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "save_uris = []\n",
    "save_boxes = []\n",
    "save_assets = []\n",
    "for i in range(progress, n_uris, batch_size):\n",
    "    batch_uris = uris[i:i+batch_size]\n",
    "    batch_assets = assets[i:i+batch_size]\n",
    "    # we call the model with the batch of uris\n",
    "    calls = [asyncio.wrap_future(input_unit(run_info=gen_message(url=uri))) for uri in batch_uris]\n",
    "    \n",
    "    results = await asyncio.gather(*calls)\n",
    "    \n",
    "    # we need to get the boxes now\n",
    "    batch_boxes = []\n",
    "    for res in results:\n",
    "        frame_detections = res[0][\"frame_detections\"]\n",
    "        detections = frame_detections.detections\n",
    "        boxes = []\n",
    "        if detections:\n",
    "            for frame_detection in detections:\n",
    "                score = frame_detection.classification[0][-1]\n",
    "                box = frame_detection.box\n",
    "                box = transf_any_box(box, \"xyxy\", \"xywh\")\n",
    "                boxes.append({\"score\": score, \"box\": box})\n",
    "                \n",
    "        batch_boxes.append(boxes)\n",
    "    \n",
    "    save_uris += batch_uris\n",
    "    save_boxes += batch_boxes\n",
    "    save_assets += batch_assets\n",
    "    \n",
    "    \n",
    "    if (i + batch_size) % 20000 == 0:\n",
    "        \n",
    "        save_batch(save_assets, save_uris, save_boxes, i+batch_size)\n",
    "\n",
    "        # reset the batch count, save uris, and save boxes\n",
    "        save_uris = []\n",
    "        save_boxes = []\n",
    "        save_assets = []\n",
    "        \n",
    "# Handle the final batch\n",
    "if save_uris:\n",
    "    save_batch(save_assets, save_uris, save_boxes, n_uris)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
