{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install humanize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"/Workspace/Users/pdacosta@integralads.com/.ide/ctx-inference_stack/src\")\n",
    "sys.path.append(\"/Workspace/Users/pdacosta@integralads.com/.ide/ctx-logoface-detector-a4cd2ffb\")\n",
    "os.environ[\"AWS_PROFILE\"]=\"saml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compute_engine.units.generator import UnitGenerator, UnitDeclatation\n",
    "from compute_engine.structures.entities import RunInfo\n",
    "from compute_engine.structures.messages import MessageHeader, RunInfoMessage, EndOfComputeMessage\n",
    "from compute_engine.structures.entities import FrameDetection, FrameDetections\n",
    "from compute_engine.units.handler import Handler, RunVariable\n",
    "from compute_engine.utils.message_capture import MessageCapture\n",
    "\n",
    "from concurrent.futures import Future\n",
    "import pyarrow.parquet as pq\n",
    "import asyncio\n",
    "import pyarrow as pa\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import concat, lit, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers = 2\n",
    "model_batch_size = 16\n",
    "detection_min_size_percentage = 0.01\n",
    "confidence_threshold = 0.0\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "generate_uris = True\n",
    "ignore_progress = True\n",
    "\n",
    "dbfs_mnt_path_faces = '/dbfs/mnt/innovation/pdacosta/data/wider_face/ias/faces'\n",
    "dbfs_mnt_path_uris = '/dbfs/mnt/innovation/pdacosta/data/wider_face/ias/uris'\n",
    "\n",
    "pyspark_mnt_path_faces = dbfs_mnt_path_faces.replace('/dbfs', '')\n",
    "pyspark_mnt_path_uris = dbfs_mnt_path_uris.replace('/dbfs', '')\n",
    "\n",
    "csv_mnt_path = \"/dbfs/mnt/innovation//pdacosta/data/wider_face/dataset/\"\n",
    "pyspark_csv_mnt_path = csv_mnt_path.replace('/dbfs', '')\n",
    "\n",
    "s3_prefix = \"s3://mls.us-east-1.innovation/pdacosta/data/wider_face/dataset\"\n",
    "\n",
    "progress_file = os.path.join(dbfs_mnt_path_uris, 'progress.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "def gen_message(url):\n",
    "    global i\n",
    "    run_info = {}\n",
    "    run_info[\"export\"] = 'json'\n",
    "    run_info[\"pipeline\"] = {\"mode\": \"semi_auto\"}\n",
    "    run_info[\"source\"] = {\n",
    "        \"kind\": \"image\",\n",
    "        \"url\": url,\n",
    "        \"uuid\": \"0\"\n",
    "    }\n",
    "    run_info[\"run\"] = {\n",
    "        \"id\": i\n",
    "    }\n",
    "    i += 1\n",
    "    run_info[\"company\"] = {\"id\": 0}\n",
    "    run_info = RunInfo(**run_info, atomic=None)\n",
    "    return run_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_generator = UnitGenerator(model_engine=\"tf\")\n",
    "unit_generator.set_config(\"database_client\", None)\n",
    "unit_generator.set_config(\"bucket_name\", \"reminiz.production\")\n",
    "unit_generator.set_config(\"cloud_provider\", \"aws\")\n",
    "unit_generator.set_config(\"models_local_path\", \"./\")\n",
    "unit_generator.set_config(\"s3_bucket_url\", \"\")\n",
    "\n",
    "\n",
    "units = []\n",
    "with unit_generator:\n",
    "    input_unit = unit_generator.units.downloader()\n",
    "    x = input_unit\n",
    "    units.append(x)\n",
    "    x @= unit_generator.units.run_frame_extractor()\n",
    "    units.append(x)\n",
    "    x @= unit_generator.units.frame_resizer(target_size=416)\n",
    "    units.append(x)\n",
    "    x @= unit_generator.units.detector(\n",
    "        max_workers= n_workers,\n",
    "        model_path= \"models/detectors/faces/face_detector_march_20-20230116-tf/\",\n",
    "        batchsize= model_batch_size,\n",
    "        detection_min_size_percentage= detection_min_size_percentage,\n",
    "        confidence_threshold=confidence_threshold\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_uris:\n",
    "    \n",
    "    # read the csv files with the annotations\n",
    "    val_df = spark.read.csv(os.path.join(pyspark_csv_mnt_path, \"val.csv\"), header=True)\n",
    "    \n",
    "    # stack the dataframes\n",
    "    df = val_df\n",
    "    \n",
    "    # create the correct uri for the images, we need to add the s3 prefix to the paths\n",
    "    df = df.withColumn(\"uri\", concat(lit(s3_prefix), col(\"path\")))\n",
    "    \n",
    "    # filter out the images that are too small, cannot have a dimension smaller than 96 px\n",
    "    df = df.filter((col(\"width\") >= 96) & (col(\"height\") >= 96))\n",
    "    \n",
    "    #keep only the columns we need\n",
    "    df = df.select(\"asset\", \"uri\")\n",
    "    \n",
    "    # read the assets already saved\n",
    "    # if the file does not exist, avoid the error\n",
    "    try:\n",
    "        assets_saved = spark.read.parquet(pyspark_mnt_path_faces)\n",
    "        assets_saved = assets_saved.select(\"asset\").distinct()\n",
    "        # filter out the assets already saved\n",
    "        df = df.join(assets_saved, on=\"asset\", how=\"left_anti\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # make sure we have no duplicates\n",
    "    df = df.dropDuplicates([\"asset\"])\n",
    "    \n",
    "    #save df as a csv file \n",
    "    df.write.mode(\"overwrite\").csv(os.path.join(pyspark_mnt_path_uris, \"uris.csv\"), header=True)\n",
    "        \n",
    "else:\n",
    "    \n",
    "    df = spark.read.csv(os.path.join(pyspark_mnt_path_uris, \"uris.csv\"), header=True)\n",
    "    \n",
    "\n",
    "# collect the uris and assets into a python lists\n",
    "rows = df.collect()\n",
    "assets, uris = [row.asset for row in rows], [row.uri for row in rows]\n",
    "df.unpersist()\n",
    "\n",
    "print(f\"Number of images to process: {len(assets)}\")\n",
    "print(f\"Assets: {assets[:2]}\")\n",
    "print(f\"Uris: {uris[:2]}\")    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
