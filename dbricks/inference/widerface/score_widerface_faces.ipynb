{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install humanize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(\"/Workspace/Users/pdacosta@integralads.com/.ide/ctx-inference_stack/src\")\n",
    "sys.path.append(\"/Workspace/Users/pdacosta@integralads.com/.ide/ctx-logoface-detector-a4cd2ffb\")\n",
    "os.environ[\"AWS_PROFILE\"]=\"saml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compute_engine.units.generator import UnitGenerator, UnitDeclatation\n",
    "from compute_engine.structures.entities import RunInfo\n",
    "from compute_engine.structures.messages import MessageHeader, RunInfoMessage, EndOfComputeMessage\n",
    "from compute_engine.structures.entities import FrameDetection, FrameDetections\n",
    "from compute_engine.units.handler import Handler, RunVariable\n",
    "from compute_engine.utils.message_capture import MessageCapture\n",
    "\n",
    "from concurrent.futures import Future\n",
    "import pyarrow.parquet as pq\n",
    "import asyncio\n",
    "import pyarrow as pa\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import concat, lit, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_workers = 2\n",
    "model_batch_size = 16\n",
    "detection_min_size_percentage = 0.01\n",
    "confidence_threshold = 0.4\n",
    "\n",
    "\n",
    "batch_size = 16\n",
    "generate_uris = True\n",
    "ignore_progress = True\n",
    "\n",
    "dbfs_mnt_path_faces = '/dbfs/mnt/innovation/pdacosta/data/wider_face/preds_faces/xywh'\n",
    "dbfs_mnt_path_uris = '/dbfs/mnt/innovation/pdacosta/data/wider_face/preds_faces/xywh/uris'\n",
    "\n",
    "pyspark_mnt_path_faces = dbfs_mnt_path_faces.replace('/dbfs', '')\n",
    "pyspark_mnt_path_uris = dbfs_mnt_path_uris.replace('/dbfs', '')\n",
    "\n",
    "csv_mnt_path = \"/dbfs/mnt/innovation/pdacosta/data/wider_face/dataset/\"\n",
    "pyspark_csv_mnt_path = csv_mnt_path.replace('/dbfs', '')\n",
    "\n",
    "s3_prefix = \"s3://mls.us-east-1.innovation/pdacosta/data/wider_face/dataset/\"\n",
    "\n",
    "progress_file = os.path.join(dbfs_mnt_path_uris, 'progress.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "def gen_message(url):\n",
    "    global i\n",
    "    run_info = {}\n",
    "    run_info[\"export\"] = 'json'\n",
    "    run_info[\"pipeline\"] = {\"mode\": \"semi_auto\"}\n",
    "    run_info[\"source\"] = {\n",
    "        \"kind\": \"image\",\n",
    "        \"url\": url,\n",
    "        \"uuid\": \"0\"\n",
    "    }\n",
    "    run_info[\"run\"] = {\n",
    "        \"id\": i\n",
    "    }\n",
    "    i += 1\n",
    "    run_info[\"company\"] = {\"id\": 0}\n",
    "    run_info = RunInfo(**run_info, atomic=None)\n",
    "    return run_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_generator = UnitGenerator(model_engine=\"tf\")\n",
    "unit_generator.set_config(\"database_client\", None)\n",
    "unit_generator.set_config(\"bucket_name\", \"reminiz.production\")\n",
    "unit_generator.set_config(\"cloud_provider\", \"aws\")\n",
    "unit_generator.set_config(\"models_local_path\", \"./\")\n",
    "unit_generator.set_config(\"s3_bucket_url\", \"\")\n",
    "\n",
    "\n",
    "units = []\n",
    "with unit_generator:\n",
    "    input_unit = unit_generator.units.downloader()\n",
    "    x = input_unit\n",
    "    units.append(x)\n",
    "    x @= unit_generator.units.run_frame_extractor()\n",
    "    units.append(x)\n",
    "    x @= unit_generator.units.frame_resizer(target_size=416)\n",
    "    units.append(x)\n",
    "    x @= unit_generator.units.detector(\n",
    "        max_workers= n_workers,\n",
    "        model_path= \"models/detectors/faces/face_detector_march_20-20230116-tf/\",\n",
    "        batchsize= model_batch_size,\n",
    "        detection_min_size_percentage= detection_min_size_percentage,\n",
    "        confidence_threshold=confidence_threshold\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_uris:\n",
    "    \n",
    "    # read the csv files with the annotations\n",
    "    val_df = spark.read.csv(os.path.join(pyspark_csv_mnt_path, \"val.csv\"), header=True)\n",
    "    \n",
    "    # stack the dataframes\n",
    "    df = val_df\n",
    "    \n",
    "    # create the correct uri for the images, we need to add the s3 prefix to the paths\n",
    "    df = df.withColumn(\"uri\", concat(lit(s3_prefix), col(\"path\")))\n",
    "    \n",
    "    # filter out the images that are too small, cannot have a dimension smaller than 96 px\n",
    "    df = df.filter((col(\"width\") >= 96) & (col(\"height\") >= 96))\n",
    "    \n",
    "    #keep only the columns we need\n",
    "    df = df.select(\"asset\", \"uri\")\n",
    "    \n",
    "    # read the assets already saved\n",
    "    # if the file does not exist, avoid the error\n",
    "    try:\n",
    "        assets_saved = spark.read.parquet(pyspark_mnt_path_faces)\n",
    "        assets_saved = assets_saved.select(\"asset\").distinct()\n",
    "        # filter out the assets already saved\n",
    "        df = df.join(assets_saved, on=\"asset\", how=\"left_anti\")\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # make sure we have no duplicates\n",
    "    df = df.dropDuplicates([\"asset\"])\n",
    "    \n",
    "    #save df as a csv file \n",
    "    df.write.mode(\"overwrite\").csv(os.path.join(pyspark_mnt_path_uris, \"uris.csv\"), header=True)\n",
    "        \n",
    "else:\n",
    "    \n",
    "    df = spark.read.csv(os.path.join(pyspark_mnt_path_uris, \"uris.csv\"), header=True)\n",
    "    \n",
    "\n",
    "# collect the uris and assets into a python lists\n",
    "rows = df.collect()\n",
    "assets, uris = [row.asset for row in rows], [row.uri for row in rows]\n",
    "df.unpersist()\n",
    "\n",
    "print(f\"Number of images to process: {len(assets)}\")\n",
    "print(f\"Assets: {assets[:2]}\")\n",
    "print(f\"Uris: {uris[:2]}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = input_unit(\n",
    "    run_info=gen_message(\n",
    "        url=uris[0]\n",
    "    )\n",
    ")\n",
    "f.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_batch(assets, uris, boxes, idx):\n",
    "    \n",
    "    \n",
    "    data = [{\"asset\":asset, \"uri\": uri, \"boxes\": boxes} for asset, uri, boxes in zip(assets, uris, boxes)]\n",
    "    \n",
    "    df_ = pd.DataFrame(data)\n",
    "    table = pa.Table.from_pandas(df_)\n",
    "    parquet_filename = os.path.join(dbfs_mnt_path_faces, f\"face_boxes_{str(idx).zfill(6)}.parquet\")\n",
    "    try:\n",
    "        pq.write_table(table, parquet_filename)\n",
    "        with open(progress_file, 'w') as f:\n",
    "            f.write(str(idx))\n",
    "    except:\n",
    "        raise Exception('Could not save the parquet file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not ignore_progress:\n",
    "    # read the progress index from a file\n",
    "    try:\n",
    "        with open(progress_file, 'r') as f:\n",
    "            progress = int(f.read())\n",
    "    except:\n",
    "        progress = 0\n",
    "else:\n",
    "    progress = 0\n",
    "\n",
    "n_uris = len(uris)\n",
    "print(f'Progress: {progress} out of {n_uris} images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_uris = []\n",
    "save_boxes = []\n",
    "save_assets = []\n",
    "for i in range(progress, n_uris, batch_size):\n",
    "    batch_uris = uris[i:i+batch_size]\n",
    "    batch_assets = assets[i:i+batch_size]\n",
    "    # we call the model with the batch of uris\n",
    "    calls = [asyncio.wrap_future(input_unit(run_info=gen_message(url=uri))) for uri in batch_uris]\n",
    "    \n",
    "    results = await asyncio.gather(*calls)\n",
    "    \n",
    "    # we need to get the boxes now\n",
    "    batch_boxes = []\n",
    "    for res in results:\n",
    "        frame_detections = res[0][\"frame_detections\"]\n",
    "        detections = frame_detections.detections\n",
    "        boxes = []\n",
    "        if detections:\n",
    "            for frame_detection in detections:\n",
    "                score = frame_detection.classification[0][-1]\n",
    "                box = frame_detection.box\n",
    "                box = transf_any_box(box, \"xyxy\", \"xywh\")\n",
    "                boxes.append({\"score\": score, \"box\": box})\n",
    "                \n",
    "        batch_boxes.append(boxes)\n",
    "    \n",
    "    save_uris += batch_uris\n",
    "    save_boxes += batch_boxes\n",
    "    save_assets += batch_assets\n",
    "    \n",
    "    \n",
    "    if (i + batch_size) % 20000 == 0:\n",
    "        \n",
    "        save_batch(save_assets, save_uris, save_boxes, i+batch_size)\n",
    "\n",
    "        # reset the batch count, save uris, and save boxes\n",
    "        save_uris = []\n",
    "        save_boxes = []\n",
    "        save_assets = []\n",
    "        \n",
    "# Handle the final batch\n",
    "if save_uris:\n",
    "    save_batch(save_assets, save_uris, save_boxes, n_uris)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
